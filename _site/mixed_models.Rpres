Linear Mixed Models
========================================================
incremental: true

![mixed](mixed-model.jpg)

Categorical Explanatory Variables
=====================

So far in [ANOVA](anova.html) we have treated all categorical vars as the same

There are actually two types of categorical variables
 
*  ***fixed effects***
*  ***random effects***

## We need to deal with random effects differently

## It can be tricky at first to tell them apart
 
Telling them apart
==================

ANOVA model: $$Y_{ij}=\mu + A_i + \epsilon_{ij}$$

**Fixed effects**: ($A_i$) affect the mean of groups in a meaningful way

Mixed Model: $$Y_{ij}=\mu + A_i + U_i + \epsilon_{ij}$$

**Random effects**: ($U_i$) structure variance, but not in an additive way like fixed effects. Think of these as additional, structured error terms


Telling them apart
=============

**Fixed Effects**:

*  meaningful factor labels (e.g., male)
*  small possible range of values....you have data for all of them
*  affect only mean of $y$ variable
*  effect predictable in different studies (e.g., sex in monkey morphometrics)

*** 

**Random Effects**:

*  uninformative factor labels (e.g., site A)
*  very large range of possible values...you have data for a random sample of them
*  affect only the variance of $y$ variable
*  effect unpredictable or meaningless across studies

Telling them apart - Example
=============
incremental: false

**Question: do aggression rates differ by sex?**

You follow a group of habituated monkeys for 2 months. Each day you watch each monkey for an hour, and record the number of agonistic encounters, and the time of day the observation was made.  In the end you have `r 10 * 60` observations (10 monkeys $\times$ 60 days).


MonkeyID | Sex | AggEncounters | ObservationTime
---|----|----|----
Bob | male | 4 | morning
Cindy | female | 3 | morning
Bob | male | 7 | evening
Cindy | female | 2 | evening
... | ... | ... | ...

Telling them apart - Example
=============

MonkeyID | Sex | AggEncounters | ObservationTime
---|----|----|----
Bob | male | 4 | morning
Cindy | female | 3 | morning
Bob | male | 7 | evening
Cindy | female | 2 | evening
... | ... | ... | ...

You have a single response variable: **AggEncounters**

Three predictor variables: **Sex**, **MonkeyID**, **ObservationTime**

**Question: do aggression rates differ by sex?**

Which are fixed and which are random effects?

Two main circumstances where LMM is first choice
================

*  none of your effects are fixed (less common)
*  you have one or more fixed effects, but also have ***pseudo-replication*** (more common)

Pseudo-replication
==================

Recall from our discussion of ANOVA that independent observations in a treatment group are called ***replicates***

***Pseudoreplication*** occurs when something masquerading as a replicate of the treatment actually isn't independent  

This situation is common, and is easy to identify with a bit of practice. 

Pseudo-replication
==================

There are two major kinds of pseudo-replication:

*  temporal pseudoreplication - typically involves ***repeated*** measures on same individual
*  spatial pseudoreplication - typically involves observations that may be similar due to spatial association


***Challenge:*** give me some examples of pseudoreplicated data from biological anthropology.

Pseudo-replication
==================

MonkeyID | Sex | AggEncounters | ObservationTime
---|----|----|----
Bob | male | 4 | morning
Cindy | female | 3 | morning
Bob | male | 7 | evening
Cindy | female | 2 | evening
... | ... | ... | ...

Recall our dataset of 600 observations of sex and aggressive encounter rate, measured daily for 2 months (60 days) for 10 individuals.  

**This dataset is massively pseudo-replicated.....why?**

Steps in Linear Mixed Modelling (LMM)
===================

*  decide on the structure of your model (which are fixed and which are random effects)
*  identify any nesting structure 
*  specify the formula correctly, 

LMM in R
====================

`lme4` package

```{r}
library(lme4)
```

Models are specified much like normal linear models

Random effects are specified as: `(1|randomEffect)`

The 1 stands in for the intercept.

Effectively, you are saying: "allow each level of `randomEffect` to have its own independent intercept"

LMM in R
====================
incremental: false
left: 60

```{r}
library(lme4)
head(sleepstudy)
```

***

```{r echo=FALSE}
library(ggplot2)
qplot(x=Days, y=Reaction, color=Subject, geom="line", data=sleepstudy) + 
  geom_point() + 
  theme_bw(30)
```


LMM in R
=================

One option is to fit a model with a random effect term to allow each subject to have its own intercept:

```{r}
randIntercept <- lmer(Reaction ~ Days + (1 | Subject), sleepstudy)
summary(randIntercept)
```

LMM in R
=================

Another option is to fit a model allowing both the slope and the intercept to vary for each subject:

```{r}
randSlopeInt <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
summary(randIntercept)
```

LMM in R - Is my model any good?
===========

One tool is the Akaike Information Criterion (AIC)

Quantifies goodness of fit, while penalizing for model complexity

```{r}
AIC(randIntercept)
AIC(randSlopeInt)
```

***

![golf](golf.jpg)

## The lower the AIC the better

Model Simplification
===================
type: section

## "Everything should be kept as simple as possible, but no simpler."

### - Albert Einstein (probably never said this...)


![einstein](einstein.jpg)

Example - Isler et al. (2008)
===========

```{r}
brains <- read.table("http://hompal-stats.wabarr.com/datasets/Isler_et_al_brains.txt", header=TRUE, sep="\t")
library(dplyr)
brains <- brains %>% 
  select(Species, ECV..cc., Body.mass..g., Wild.captive) %>%
  filter(Wild.captive %in% c("Wild", "Captive"))

head(brains)
```

Example - Isler et al. (2008)
===========

Simplest model just uses body mass

```{r}
mod1 <- lm(log(ECV..cc.) ~ log(Body.mass..g.), data=brains)
```

Slightly more complicated model includes wild versus captive

```{r}
mod2 <- lm(log(ECV..cc.) ~ log(Body.mass..g.) + Wild.captive, data=brains)
```

***Which model is better?*** In this simple case, we can look at significance of individual terms, and $R^2$, but in LMM we don't have this option.


anova() function - new use for old friend 
=======================

Recall that `anova()` doesn't do analysis of variance, it creates an ANOVA table from a model

You can also compare two models with `anova()` to test the hypothesis that they are significantly different

The models are compared with a ***likelihood-ratio test***

likelihood ratio test
===================

Only valid for models that are ***nested***: *i.e., one is a subset of the other*

```{}
simpler  <-    lm(resp ~ fac1 + fac2)
morecomplex <- lm(resp ~ fac1 + fac2 + fac3)
anova(simpler, morecomplex)
```

***Note:*** more complex models *ALWAYS* fit the data better, but likelihood ratio test asks if this difference is significant

Example - Isler et al. (2008)
===========

```{r}
anova(mod1, mod2)
```

Generalized Linear Mixed Models
==================

Like any other linear models, a basic assumption of general linear mixed-models is a normal error term

However, the structure of data may make this assumption invalid (e.g. binary data or count data)

Two very common types of non-normal error structures are:

*  poisson - for count data
*  binomial - for binary (e.g. presence absence data) or proportion data

GLMM in R
==================

```{}
lmer(response ~ fixedEffect + (1|randomEffect), family="poisson")
```

Regression
========================================================
incremental: true
left:10%

***
<img src="http://imgs.xkcd.com/comics/sustainable.png"> 


Simple Linear Model
===========================

Simplest way two variables can be modeled as related to one another

$$Y_i = \beta_0 + \beta_1X_i + \epsilon_i$$

*  $\beta_0$ is the **intercept** (value of y where x= 0)
*  $\beta_1X_i$ is the **slope** value expressing $\Delta Y / \Delta X$
*  $\epsilon_i$ is the error term

Simple Linear Model
===========================
incremental:false

```{r echo=FALSE}
library(ggplot2)
predictor <- rnorm(100, mean=100, sd=10)
error <- rnorm(100, mean=0, sd=3)
response <- 25 + predictor * 0.3 + error
qplot(predictor, response) + theme_bw(20)
```

Finding the best line
======================
```{r echo=FALSE}
plotdat <- data.frame(meanresp = mean(response), meanpred=mean(predictor))
qplot(predictor, response) + theme_bw(20) + geom_point(data=plotdat, aes(x=meanpred, y=meanresp), size=8, color="red")
```

***

Once we decide on linear regression, we have to find best line

There is unexplained variation in Y (why?)

Many lines pass through $(\bar{X},\bar{Y})$.  How do we pick the best one?


Finding the best line
======================

The best line minimizes the **residual sum of squares**

The **squared residual** represents how far off the predicted value a given point is: $$d_i^2=(Y_i - \hat{Y}_i)^2$$

Therefore: $$ RSS=\sum\limits_{i=1}^n(Y_i - \hat{Y}_i)^2$$

We could try a bunch of slopes passing through $(\bar{X},\bar{Y})$ and calculate the RSS, then pick the smallest, but math offers a better solution.

Variances and Covariances
=======================
Recall the **sum of squares**:  $$\ SS_Y = \sum\limits_{i=1}^n(Y_i - \bar{Y})^2$$

**Sample variance**:  $$\ s^2_Y=\frac{\sum\limits_{i=1}^n(Y_i - \bar{Y})^2}{n-1}$$

SS equivalent to: $$\ SS_Y = \sum\limits_{i=1}^n(Y_i - \bar{Y})(Y_i - \bar{Y})$$

Variances and Covariances
=======================
With 2 variables, we can define **sum of cross products** $$\ SS_{XY} = \sum\limits_{i=1}^n(X_i - \bar{X})(Y_i - \bar{Y})$$

By analogy to the sample variance, we define **sample covariance** $$\ s_{XY} = \frac{\sum\limits_{i=1}^n(X_i - \bar{X})(Y_i - \bar{Y})}{(n-1)}$$

Sample covariance from $-\infty$ to $\infty$

Calculate Covariance
=======================
By hand on whiteboard

```{r}
x <- c(2 , 4, 3)
y <- c(1, 5, 6)
```

```{r}
cov(x,y)
```

Regression Parameters - Slope
=================

In **ordinary least squares regression (OLS)**, the slope of the best fit line is defined as: $$ \frac{covariance\ of\ XY}{variance\ of\ X}$$

Or: $$\hat{\beta}_1 = \frac{s_{XY}}{s^2_X} = \frac{SS_{XY}}{SS_X}$$ **note: this simplifies because both are divided by same denomenator**

Regression Parameters - Intercept
=================

The intercept is easy to find, because the best fit line passes through $(\bar{X},\bar{Y})$. Thus:
$$ \bar{Y} =   \hat{\beta}_0 + \hat{\beta}_1 \bar{X} $$

Or:

$$ \hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}$$

Regression Parameters - Error
===================

Recall: $$Y_i = \beta_0 + \beta_1X_i + \epsilon_i$$

Regression assumes that that $\epsilon$ is a random normal variable with mean 0 and variance of $\sigma^2$, which is related to the scatter around the line. 

We estimate $\sigma^2$ like this: $$\frac{RSS}{n-2}$$

The square root of this is **standard error of regression**: $$\hat{\sigma}^2=\sqrt{\frac{RSS}{n-2}}$$

Coefficient of Determination
=====================

In a linear relationship, the total variance we want to explain is $SS_Y$.

Some variance is attribituable to our error term (measured by $RSS$).  

The remaining variance is explained by our regression, thus: $$SS_{reg}=SS_Y-RSS$$

Therefore: $$SS_Y=SS_{reg}+RSS$$ 

This is refered to as **partitioning** a sum of squares.

Coefficient of Determination
=====================

This leads to calculation of $r^2$, AKA the **coefficient of determination**

$$\frac{SS_{reg}}{SS_Y}=\frac{SS_{reg}}{SS_{reg} + RSS}$$

The square root of this value is known as $r$ or the *product-moment correlation coefficient*. Note: the sign of $r$ comes from the sign of the slope of the line. 

Hypothesis Testing
=====================

You will always get parameter estimates for the intercept and slope.  The next question is: **are they signficant?**

The slope measures the strength of the effect of $X$ on $Y$.  The slope is a measure of **effect size**

## What is the null hypothesis about the slope in regression? 

Hypothesis Testing ANOVA tables
=====================
incremental:false

Source | Degrees of Freedom (df) | Sum of squares (SS) | Mean Square (MS) | Expected Mean Square | F-ratio | P-value
----|----|----|----|----|----|----
Regression|1|$SS_{reg}=\sum\limits_{i=1}^n (\hat{Y}_i-\bar{Y})^2$|$\frac{SS_{reg}}{1}$|$\sigma^2 + \beta_i^2 \sum\limits_{i=1}^2 X_i^2$|$\frac{SS_{reg}/1}{RSS/(n-2)}$|F dist
Residual|n-2|$RSS=\sum\limits_{i-1}^n(Y_i - \hat{Y}_i)^2$|$\frac{RSS}{(n-2)}$|$\sigma^2$

*  $\bar{Y}$ = mean of Y
*  $\hat{Y}_i$ = predicted value from regression
*  $Y_i$ = value of Y

Confidence Intervals
======================
incremental:false

We can also construct confidence intervals around our parameter estimates (formulae not shown).

Notice that our intervals are narrowest near the mean, and fatter the further you go away from mean.

Prediction intervals are related, but slightly wider.

***

```{r echo=FALSE}
qplot(predictor, response) + theme_bw(20) + stat_smooth(method="lm")
```

Interpolation vs Extrapolation
=====================

```{r echo=FALSE}
x <- runif(50, 0, 2.5)
y <- x^3 - 3*x
thePlot <- qplot(x,y, size=I(5)) + theme_bw(20)

interval <-which(x >=1.1 & x<1.8)
mod <- summary(lm(y[interval]~x[interval]))
thePlot + geom_abline(slope=mod$coef[2], intercept=mod$coef[1], linetype="dashed", size=2, color="red") + scale_x_continuous(limits=c(1.1, 1.8)) + scale_y_continuous(limits=c(-2, 0.2))
```
***
```{r echo=FALSE}
thePlot + geom_abline(slope=mod$coef[2], intercept=mod$coef[1], linetype="dashed", size=2, color="red") + geom_vline(xintercept=1.8) + geom_vline(xintercept=1.1)
```

Assumptions of Regression
==============

1.  The causal relationship between X and Y is linear.
2.  The X variable is measured without error.
      * Datasets that don't meet this require Model II (a.k.a., RMA)
3.  Y values are independent with normally distributed errors
4.  Variance is constant along the regression line (homoscedasticity)

Checking Assumptions: Residual plot
======================
incremental:False
left: 75

```{r}
plot(lm(response~predictor))
```

***

Should look like the stars in the night sky!

Any systematic correlation between the residuals and the fitted is bad.

Checking Assumptions: Residual plot
======================
incremental:False
left: 75

```{r}
response <- predictor^2 + error*20
plot(lm(response~predictor))
```

***

Systematic departures from linearity can show up in the residual plots

Doing Regression in R
====================

```{r results="hide"}
myModel <- lm(response~predictor)
summary(myModel)
```

Doing Regression in R
====================
left:90
incremental:false

```{r}
plot(myModel)
```

***

Doing Model II Regression in R
====================
incremental:false

Model II (aka RMA) regression requires a separate package `lmodel2`

The function is different, but the call is still simple.  

```{r results="hide"}
library(lmodel2)
lmodel2(response~predictor)
```


Regression In Context
==========================
type:section

Regression is just one (the simplest) of many types of linear models that can describe linear relationships.  

The bigger group comprises the

## General Linear Model


General Linear Model
=======================

Family of statistical models of the form:

$$Y_i = \beta_0 + \beta_1X_i + \beta_2X_i +\ ... +\ \beta_nX_i  + \epsilon_i$$

where: 

* $\beta_0$ is the y-intercept (value of y where x= 0)
*  $\beta_1X_i$ is the slope value for the 1st x variable
*  $\epsilon_i$ is the error term


General Linear Model
=======================

Includes many common statistical methods:

*  Regression
*  Multiple Regression
*  ANOVA
*  ANCOVA

**All of these use the same function in R, called `lm()`**

Choosing a General Linear Model
=======================

First decide which are explanatory and response variables. 

*  All explanatory variables continuous
    *  ***Regression***
*  All explanatory variables categorical
    *  ***ANOVA (Analysis of Variance)***
*  Explanatory variables both continuous and categorical
    *  ***ANCOVA (Analysis of Covariance)***

ï¿¼

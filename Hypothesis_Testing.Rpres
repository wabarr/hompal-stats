```{r echo=FALSE}
library(ggplot2)
theme_set(theme_bw(20))
```

Hypothesis Testing
===================================================================
transition:none

Induction versus Deduction
===================================================================


***Induction*** is bottom up approach to reasoning, proceeding from specific observations to general explanations.  

***Deduction*** goes the other way: from general to specific. 

All methods of doing science use both inductive and deductive reasoning, but the emphasis that they receive differs. 

Deduction
===================================================================

![hypotheticodeductive](hypothetico_deductive.png)

Deduction 
===================================================================

*  Emphasis is on falsification
*  Requires multiple working hypotheses
*  In the end, there is (hopefully) only one that hasn't been falsified
*  Limitation: "correct" hypothesis MUST be among the alternatives studied


Induction
===================================================================

![Induction](inductive.png)


Induction
===================================================================


*  Emphasis is on confirmation
*  Builds and modifies hypothesis based on previous knowledge
*  Limitation: may "get off on wrong foot" if hypothesis is just plain wrong


Testing Hypotheses
===================================================================

Consider a [dataset of femoral head diameters](datasets/baboons.txt) from male and female baboons. 

***Read this into R on your own, and make a boxplot with the points overlaid on the boxes.*** 

We want to test the hypothesis of a relationship between sex and femoral head diamter. To do this, we first create a statistical ***null hypothesis***.

***

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
baboons <- read.table("datasets/baboons.txt", header=TRUE)


qplot(x=SEX, y=FHD, data=baboons, main="Baboon FHD by Sex", geom="boxplot") + geom_point(color="red", size=3)
```

Null Hypothesis 
===================================================================

The null hypothesis is the simplest possible explanation for a phenomenon.  This explanation is usually that random variation is responsible for any apparent pattern.

In the case of our baboons,  the null hypothesis is that femoral head diameter is in NO way related to sex, and that any apparent association between the two is due to random chance.  

Alternatives to the Null
===================================================================

Next, we create one or more statistical alternative hypotheses.  In our baboon case, the alternative is that the difference between male and female femoral head diameters is too great to be accounted for by chance alone. 

Most of the time, we don't explicitly specify the alternative, we just suffice it so say that the alternative is "not $H_0$"

<span class="mega-octicon octicon-alert"></span> The alternative hypothesis is simply focussed on the pattern in the data...not the cause of it. 

The all powerful p-value
===================================================================

***The p-value is an estimate of how likely our data are, assuming the null hypothesis is true.***

In terms of conditional probability, the p value represents 

$$ P(Data|H_0) $$

The all powerful p-value
===================================================================


In the case of our baboons, we would probably use a T-test to compare group means. Like all parametric statistics, the T-test returns a ***test statistic*** that is in this case called T, and measures how far the group means are from one another. 

Because we are assuming that the null hypothesis is true for the moment, we can compute a disribution of how likely it is to get various values of T when there are no differences between groups.

The all powerful p-value
===================================================================

```{r echo=FALSE}
library(ggplot2)
xrange <- seq(-15, 15, 0.05)
qplot(x=xrange, y=dt(xrange, df = 1000), geom="line",  main="Density of T-Distribution df=1000")

```

***

```{r echo=FALSE}
qplot(x=xrange, y=dt(xrange, df = 24), geom="line",  main="Density of T-Distribution df=23") +
  geom_vline(xintercept = -12.4224, col="red") + 
  annotate(geom="text", x=-9, y=.3, label="Observed T", color="red")
```

The all powerful p-value
===================================================================
incremental: true

Our T-test would return a very low p value, because it would be highly unlikely to get two sex groups with means as different as our baboons, if the variation was attributable only to chance. 

We know this because we know the distribution of the T statitic when the null hypothesis if true. 

If you only remember one thing...
===================================================================
type:alert
<br>
The p-value is an estimate of how likely our data are, assuming the null hypothesis is true.

Type I and Type II Errors.
===================================================================

The Truth | Retain $H_0$ | Reject $H_0$
---------|----------------|---------
$H_0$ True | Correct! | Type I error ($\alpha$)
$H_0$ False | Type II error ($\beta$)| Correct!

![errors](typeItype2.jpg)

Type I and Type II Errors.
===================================================================

***Statistical power*** is related to Type II errors, and is calculated as $1 - \beta$. 

This tells us how likely we are to detect an effect when one actually exists. 

Before starting a study it is worth doing a ***power analysis*** to determine the rate at which an effect of a given size will be detected with a given sample size. 

Type I and Type II Errors.
===================================================================

Type I and Type II error rates are necessarily inversely related to one other, so to decrease one is to increase the other. 

This relationship is not simple, though. Type II error rates depend on a lot of things, like the sample size and strength of the effect.

Statistical Significance versus Biological Significance
===================================================================

Given very large sample sizes, even tiny, virtually meaningless differences can become statistically signficant, because the statistical power increases as sample size increases. 

It is therefore important (especially in the days of easy access to lots of data) to be careful how you interpret statistical significance.  

Statistical Significance versus Biological Significance
===================================================================

For example, imagine that you showed with vast sample sizes that two populations differ in height  by an average of 0.6mm, and your p value was 0.00002.  

This is a very statistically signficant result, but it is unclear if this diference is large enough to have much biological meaning for the two populations. 

***The overwhelming take-away message of this statistical test should be that the populations are almost the same.***  

## Don't be a slave to $p<0.05$ !!!!!


Three Hypothesis Testing Frameworks
===================================================================

*  Monte Carlo
*  Parametric
*  Bayesian

We will test the same hypothesis using the same data with all three frameworks.

Our example data
===================================================================

[Ant data](datasets/gotelli_ants.txt) from Gotelli CH 5 

***Read this data into R yourself!***

```{r echo=FALSE}
ants <- read.table("http://hompal-stats.wabarr.com/datasets/gotelli_ants.txt", header=TRUE)
```

Lets test the hypothesis that the number of ant nests in forests differs from the number of ant nests in fields. 

Monte Carlo
===================================================================

Monte Carlo refers to a world-famous casino, in a town on the French Riviera (in the Principality of Monaco).

In Monte Carlo analysis, data are randomly reshuffled over and over to specify the null hypothesis, and these reshufflings are compared against the observed data.

***

![montecarlo](http://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Whole_Monaco.jpg/600px-Whole_Monaco.jpg)

Monte Carlo
===================================================================
***There are four steps:***

1.  Decide on a test statistic
2.  By reshuffling, create a distribution of the test statistic that would be expected under the null hypothesis
3.  Decided on a one-tailed or two-tailed test
4.  Compare the observed to the null distribution and calculate the p value.

Monte Carlo - Step 1 - Test statistic
===================================================================
We will use the absolute value of the mean group means.  We will need to calculate this over and over, so we will make a function to do it.

```{r}

abs.mean <- function(ant_counts, habitats) {
  
  means <- tapply(ant_counts, habitats, 
                  FUN = mean)
  
  abs_diff <- abs(means[2] - means[1])
  return(abs_diff)
}

abs.mean(ants$n_Ants, ants$Habitat)
```

Monte Carlo - Step 2- Shuffle
===================================================================

Now, we resuffle the ant nest counts randomly with respect to the habitat category.  We can do this with the `sample()` function. ***Note:*** by default, the function draws a sample of the same size as the original vector, without replacement.

```{r}
ants$n_Ants
sample(ants$n_Ants)
```

Monte Carlo - Step 2 - Shuffle
===================================================================
We can do this 1000 times by using a for loop


```{r}
results <- numeric(1000)

for(i in 1:1000) {
  
  mix <- sample(ants$n_Ants)
  results[i] <- abs.mean(
                mix , ants$Habitat
                )
}
```

Monte Carlo - Step 2 - Shuffle
================================
We can look at the results with a histogram
***
```{r echo=FALSE}
qplot(results, main="mean differences, shuffled counts") + theme_bw(24)
```

Monte Carlo - Step 3 - one or two tailed?
===================================================================
When we compute the p value, we will ask how many  differences computed on the ramdomized data are as extreme as our actual mean difference. 

Should we look at just one side of the distribution or both?

We are looking at the absolute value, of the difference, ***so we want a 1 tailed test.***
***
```{r echo=FALSE}
qplot(results, main="mean differences, shuffled counts") + theme_bw(24)
```



Monte Carlo - Step 4 - Calculate p
===================================================================

To get our 1-tailed p-value, we just count up how many of the mean differences in the randomized data are as big as (or bigger than) our observed mean difference (3.75).

```{r}
count_extreme_diffs <- sum(results >= 3.75)
count_extreme_diffs / 1000

```

Monte Carlo 
============================================================

## ***What does our p value of approx. 0.03 mean?***




Monte Carlo 
============================================================

## ***What does our p value of approx. 0.03 mean?***

If there is no relationship between habitat and and nest count, we would expect to observe data with a mean difference as extreme as ours about 3% of the time. 

Monte Carlo Pros and Cons
============================================================

# Pros:
*  fewest assumptions
*  null hypothesis is very clear and easy to understand

# Cons:
*  you have to "roll-your-own"
*  you don't get the same answer every time

Parametric
==================================================================
Assumes data were sampled from a specified distribution (usually normal distribution). 

Parameters of this distribution used to calculate tail probabilities data given a null hypotheis. 

$$P(data\ |\ hypothesis)$$

***

![normal](http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/700px-Normal_Distribution_PDF.svg.png)



Parametric
===================================================================

1.  Specify the test statistic
2.  Specify the null distribution
3.  Calculate the tail probability




Bayesian
===================================================================
<br>   
$$P(hypothesis\ |\ data) = \frac{P(hypothesis) * P(data\ |\ hypothesis)}{P(data)}$$
<br>    
$$Posterior\ Probability = \frac{Prior\ Probability * Likelihood}{Marginal\ Likelihood}$$

Bayesian
===================================================================
1.  Specify the hypothesis
2.  Specify the parameters as random variables
3.  Specify the priors
4.  Calculate the likelihood
5.  Calculate the posterior
6.  Interpret
Bayesian
===================================================================
Bayesian
===================================================================

A key concept in the Bayesian framework is incorporating ***prior*** knowledge into hypothesis testing.  

***
![xkcd](frequentists_vs_bayesians.png)


[xkcd]: frequentists_vs_bayesians.png
[errors]: typeItype2.jpg
[inductive]: inductive.png
[hypothetico]: hypothetico_deductive.png

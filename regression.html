<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<title></title>

<script src="libs/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.1/css/united.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.1/shim/respond.min.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-57395681-1', 'auto');
  ga('send', 'pageview');

</script>
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
<link rel="icon" href="favicon.ico" type="image/x-icon">

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="libs/highlight/textmate.css"
      type="text/css" />
<script src="libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>


<link rel="stylesheet" href="octicons/octicons.css" type="text/css" />
<link rel="stylesheet" href="css/custom-style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">

<div id="navbar">
  <div id="navbar-container">
    <ul id="navbar-ul">
      <li><a href="index.html"><span class="mega-octicon octicon-home"></a></li>
      <li><a href="datasets.html">Data</a></li>
      <li><a href="homework.html">Demos, Readings, & Homework</a></li>
      <li><a href="https://www.dropbox.com/s/5i83xqgfm6ud9td/ANTH6413_syllabus.docx?dl=0">Syllabus</a></li>
      <li><a href="gettinghelp.html">Getting Help</a></li>
      
    </ul>
  </div>
</div>



<div id="regression" class="section level1">
<h1>Regression</h1>
<p>incremental: true left:10%</p>
<hr />
<p><img src="http://imgs.xkcd.com/comics/sustainable.png"></p>
</div>
<div id="correlation-versus-causation" class="section level1">
<h1>Correlation versus Causation</h1>
<p><img src="http://www.venganza.org/images/PiratesVsTemp.png"></p>
</div>
<div id="regression-implies-x-causes-y" class="section level1">
<h1>Regression implies X causes Y</h1>
<p>left:60</p>
<p><img src="regression-figure/unnamed-chunk-1-1.png" alt="plot of chunk unnamed-chunk-1" /></p>
<hr />
<p>Both variables are continuous</p>
<p>TONS of data are suitable for this kind of analysis.</p>
<p>Examples?</p>
<p>From geometry: remember straight line formula</p>
<p><span class="math">\[Y = slope \times X + intercept\]</span></p>
</div>
<div id="simple-linear-model" class="section level1">
<h1>Simple Linear Model</h1>
<p>Simplest way two variables can be modeled as related to one another</p>
<p><span class="math">\[Y_i = \beta_0 + \beta_1X_i + \epsilon_i\]</span></p>
<ul>
<li><span class="math">\(\beta_0\)</span> is the <strong>intercept</strong> (value of y where x= 0)</li>
<li><span class="math">\(\beta_1X_i\)</span> is the <strong>slope</strong> value expressing <span class="math">\(\Delta Y / \Delta X\)</span></li>
<li><span class="math">\(\epsilon_i\)</span> is the error term
<ul>
<li>normal random variable with mean 0 and variance <span class="math">\(\sigma^2\)</span></li>
</ul></li>
</ul>
</div>
<div id="this-equation-should-make-sense" class="section level1">
<h1>This equation should make sense</h1>
<p>incremental:false left:60</p>
<p><img src="regression-figure/unnamed-chunk-2-1.png" alt="plot of chunk unnamed-chunk-2" /></p>
<hr />
<p><span class="math">\[Y_i = \beta_0 + \beta_1X_i + \epsilon_i\]</span></p>
<p>Once you decide on a line, then the value of Y equals: * the value predicted by the line, plus * a random error from our error term</p>
</div>
<div id="finding-the-best-line" class="section level1">
<h1>Finding the best line</h1>
<p>left:60</p>
<p><img src="regression-figure/unnamed-chunk-3-1.png" alt="plot of chunk unnamed-chunk-3" /></p>
<hr />
<p>Once we decide on linear regression, we have to find best line</p>
<p>There is unexplained variation in Y, so points don’t fall on straight line (why?)</p>
<p>Many lines pass through <span class="math">\((\bar{X},\bar{Y})\)</span>. How do we pick the best one?</p>
</div>
<div id="finding-the-best-line---residuals" class="section level1">
<h1>Finding the best line - residuals</h1>
<p>left:60 incremental:FALSE</p>
<p><img src="regression-figure/unnamed-chunk-4-1.png" alt="plot of chunk unnamed-chunk-4" /></p>
<hr />
<p>A <strong>residual</strong> represents the distance between the predicted value from the regression, and the actual value.</p>
<p>The <strong>squared residual</strong> is calculated as such:</p>
<p><span class="math">\[d_i^2=(Y_i - \hat{Y}_i)^2\]</span></p>
</div>
<div id="finding-the-best-line-1" class="section level1">
<h1>Finding the best line</h1>
<p>The best line minimizes the <strong>residual sum of squares</strong></p>
<p><span class="math">\[ RSS=\sum\limits_{i=1}^n(Y_i - \hat{Y}_i)^2\]</span></p>
<p>We could do a Monte Carlo approach: try a bunch of slopes passing through <span class="math">\((\bar{X},\bar{Y})\)</span> and calculate the RSS, then pick the smallest, but math offers a simpler solution.</p>
</div>
<div id="variances-and-covariances" class="section level1">
<h1>Variances and Covariances</h1>
<p>Recall the <strong>sum of squares</strong>: <span class="math">\[\ SS_Y = \sum\limits_{i=1}^n(Y_i - \bar{Y})^2\]</span></p>
<p><strong>Sample variance</strong>: <span class="math">\[\ s^2_Y=\frac{\sum\limits_{i=1}^n(Y_i - \bar{Y})^2}{n-1}\]</span></p>
<p>SS equivalent to: <span class="math">\[\ SS_Y = \sum\limits_{i=1}^n(Y_i - \bar{Y})(Y_i - \bar{Y})\]</span></p>
</div>
<div id="variances-and-covariances-1" class="section level1">
<h1>Variances and Covariances</h1>
<p>With 2 variables, we can define <strong>sum of cross products</strong> <span class="math">\[\ SS_{XY} = \sum\limits_{i=1}^n(X_i - \bar{X})(Y_i - \bar{Y})\]</span></p>
<p>By analogy to the sample variance, we define <strong>sample covariance</strong> <span class="math">\[\ s_{XY} = \frac{\sum\limits_{i=1}^n(X_i - \bar{X})(Y_i - \bar{Y})}{(n-1)}\]</span></p>
</div>
<div id="sample-covariance---negative" class="section level1">
<h1>Sample Covariance - Negative</h1>
<p>left:60 incremental: FALSE</p>
<p><img src="regression-figure/unnamed-chunk-5-1.png" alt="plot of chunk unnamed-chunk-5" /></p>
<hr />
<p>From <span class="math">\(-\infty\)</span> to <span class="math">\(\infty\)</span></p>
<p><span class="math">\[\ s_{XY} = \frac{\sum\limits_{i=1}^n(X_i - \bar{X})(Y_i - \bar{Y})}{(n-1)}\]</span></p>
</div>
<div id="sample-covariance---positive" class="section level1">
<h1>Sample Covariance - Positive</h1>
<p>left:60 incremental: FALSE</p>
<p><img src="regression-figure/unnamed-chunk-6-1.png" alt="plot of chunk unnamed-chunk-6" /></p>
<hr />
<p>From <span class="math">\(-\infty\)</span> to <span class="math">\(\infty\)</span></p>
<p><span class="math">\[\ s_{XY} = \frac{\sum\limits_{i=1}^n(X_i - \bar{X})(Y_i - \bar{Y})}{(n-1)}\]</span></p>
</div>
<div id="calculate-covariance" class="section level1">
<h1>Calculate Covariance</h1>
<p>By hand on whiteboard</p>
<pre class="r"><code>x &lt;- c(2 , 4, 3)
y &lt;- c(1, 5, 6)</code></pre>
</div>
<div id="regression-parameters---slope" class="section level1">
<h1>Regression Parameters - Slope</h1>
<p>In <strong>ordinary least squares regression (OLS)</strong>, the slope of the best fit line is defined as: <span class="math">\[ \frac{covariance\ of\ XY}{variance\ of\ X}\]</span></p>
<p>Or: <span class="math">\[\hat{\beta}_1 = \frac{s_{XY}}{s^2_X} = \frac{SS_{XY}}{SS_X}\]</span> <strong>note: this simplifies because both are divided by same denomenator</strong></p>
</div>
<div id="regression-parameters---intercept" class="section level1">
<h1>Regression Parameters - Intercept</h1>
<p>The intercept is easy to find, because the best fit line passes through <span class="math">\((\bar{X},\bar{Y})\)</span>. Thus: <span class="math">\[ \bar{Y} =   \hat{\beta}_0 + \hat{\beta}_1 \bar{X} \]</span></p>
<p>Or:</p>
<p><span class="math">\[ \hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X}\]</span></p>
</div>
<div id="regression-parameters---error" class="section level1">
<h1>Regression Parameters - Error</h1>
<p>Recall: <span class="math">\[Y_i = \beta_0 + \beta_1X_i + \epsilon_i\]</span></p>
<p>Regression assumes that that <span class="math">\(\epsilon\)</span> is a random normal variable with mean 0 and variance of <span class="math">\(\sigma^2\)</span>, which is related to the scatter around the line.</p>
<p>We estimate <span class="math">\(\sigma^2\)</span> like this: <span class="math">\[\frac{RSS}{n-2}\]</span></p>
<p>The square root of this is <strong>standard error of regression</strong>: <span class="math">\[\hat{\sigma}^2=\sqrt{\frac{RSS}{n-2}}\]</span></p>
</div>
<div id="coefficient-of-determination" class="section level1">
<h1>Coefficient of Determination</h1>
<p>In a linear relationship, the total variance we want to explain is <span class="math">\(SS_Y\)</span>.</p>
<p>Some variance is attributable to our error term (measured by <span class="math">\(RSS\)</span>).</p>
<p>The remaining variance is explained by our regression, thus: <span class="math">\[SS_{reg}=SS_Y-RSS\]</span></p>
<p>Therefore: <span class="math">\[SS_Y=SS_{reg}+RSS\]</span></p>
<p>This is referred to as <strong>partitioning</strong> a sum of squares.</p>
</div>
<div id="coefficient-of-determination-1" class="section level1">
<h1>Coefficient of Determination</h1>
<p>This leads to calculation of <span class="math">\(r^2\)</span>, AKA the <strong>coefficient of determination</strong></p>
<p><span class="math">\[\frac{SS_{reg}}{SS_Y}=\frac{SS_{reg}}{SS_{reg} + RSS}\]</span></p>
<p>The square root of this value is known as <span class="math">\(r\)</span> or the <strong>product-moment correlation coefficient</strong>.</p>
<p>Note: the sign of <span class="math">\(r\)</span> comes from the sign of the slope of the line.</p>
</div>
<div id="hypothesis-testing" class="section level1">
<h1>Hypothesis Testing</h1>
<p>You will always get parameter estimates for the intercept and slope. The next question is: <strong>are they signficant?</strong></p>
<p>The slope measures the strength of the effect of <span class="math">\(X\)</span> on <span class="math">\(Y\)</span>. The slope is a measure of <strong>effect size</strong></p>
<div id="what-is-the-null-hypothesis-about-the-slope-in-regression" class="section level2">
<h2>What is the null hypothesis about the slope in regression?</h2>
</div>
</div>
<div id="hypothesis-testing-anova-tables" class="section level1">
<h1>Hypothesis Testing ANOVA tables</h1>
<p>incremental:false</p>
<table>
<thead>
<tr class="header">
<th align="left">Source</th>
<th align="left">Degrees of Freedom (df)</th>
<th align="left">Sum of squares (SS)</th>
<th align="left">Mean Square (MS)</th>
<th align="left">Expected Mean Square</th>
<th align="left">F-ratio</th>
<th align="left">P-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Regression</td>
<td align="left">1</td>
<td align="left"><span class="math">\(SS_{reg}=\sum\limits_{i=1}^n (\hat{Y}_i-\bar{Y})^2\)</span></td>
<td align="left"><span class="math">\(\frac{SS_{reg}}{1}\)</span></td>
<td align="left"><span class="math">\(\sigma^2 + \beta_i^2 \sum\limits_{i=1}^2 X_i^2\)</span></td>
<td align="left"><span class="math">\(\frac{SS_{reg}/1}{RSS/(n-2)}\)</span></td>
<td align="left">F dist</td>
</tr>
<tr class="even">
<td align="left">Residual</td>
<td align="left">n-2</td>
<td align="left"><span class="math">\(RSS=\sum\limits_{i-1}^n(Y_i - \hat{Y}_i)^2\)</span></td>
<td align="left"><span class="math">\(\frac{RSS}{(n-2)}\)</span></td>
<td align="left"><span class="math">\(\sigma^2\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li><span class="math">\(\bar{Y}\)</span> = mean of Y</li>
<li><span class="math">\(\hat{Y}_i\)</span> = predicted value from regression</li>
<li><span class="math">\(Y_i\)</span> = value of Y</li>
</ul>
</div>
<div id="confidence-intervals" class="section level1">
<h1>Confidence Intervals</h1>
<p>incremental:false</p>
<p>We can also construct confidence intervals around our parameter estimates (formulae not shown).</p>
<p>Notice that our intervals are narrowest near the mean, and fatter the further you go away from mean.</p>
<p>Prediction intervals are related, but slightly wider.</p>
<hr />
<p><img src="regression-figure/unnamed-chunk-8-1.png" alt="plot of chunk unnamed-chunk-8" /></p>
</div>
<div id="interpolation-vs-extrapolation" class="section level1">
<h1>Interpolation vs Extrapolation</h1>
<p><img src="regression-figure/unnamed-chunk-9-1.png" alt="plot of chunk unnamed-chunk-9" /> *** <img src="regression-figure/unnamed-chunk-10-1.png" alt="plot of chunk unnamed-chunk-10" /></p>
</div>
<div id="assumptions-of-regression" class="section level1">
<h1>Assumptions of Regression</h1>
<ol style="list-style-type: decimal">
<li>The causal relationship between X and Y is linear.</li>
<li>The X variable is measured without error.
<ul>
<li>Datasets that don’t meet this require Model II (a.k.a., RMA)</li>
</ul></li>
<li>Y values are independent with normally distributed errors</li>
<li>Variance is constant along the regression line (homoscedasticity)</li>
</ol>
</div>
<div id="doing-regression-in-r" class="section level1">
<h1>Doing Regression in R</h1>
<p>incremental:FALSE</p>
<pre class="r"><code>myModel &lt;- lm(response~predictor)
summary(myModel)</code></pre>
<p><img src="example_lm_output.png"></p>
</div>
<div id="checking-assumptions-residual-plot" class="section level1">
<h1>Checking Assumptions: Residual plot</h1>
<p>incremental:FALSE left: 75</p>
<pre class="r"><code>plot(lm(response~predictor))</code></pre>
<p><img src="regression-figure/unnamed-chunk-12-1.png" alt="plot of chunk unnamed-chunk-12" /> <img src="regression-figure/unnamed-chunk-12-2.png" alt="plot of chunk unnamed-chunk-12" /> <img src="regression-figure/unnamed-chunk-12-3.png" alt="plot of chunk unnamed-chunk-12" /> <img src="regression-figure/unnamed-chunk-12-4.png" alt="plot of chunk unnamed-chunk-12" /></p>
<hr />
<p>Should look like the stars in the night sky!</p>
<p>Any systematic correlation between the residuals and the fitted is bad.</p>
</div>
<div id="checking-assumptions-residual-plot-1" class="section level1">
<h1>Checking Assumptions: Residual plot</h1>
<p>incremental:False left: 75</p>
<pre class="r"><code>response &lt;- predictor^2 + rnorm(1000, sd=50)
plot(lm(response~predictor))</code></pre>
<p><img src="regression-figure/unnamed-chunk-13-1.png" alt="plot of chunk unnamed-chunk-13" /> <img src="regression-figure/unnamed-chunk-13-2.png" alt="plot of chunk unnamed-chunk-13" /> <img src="regression-figure/unnamed-chunk-13-3.png" alt="plot of chunk unnamed-chunk-13" /> <img src="regression-figure/unnamed-chunk-13-4.png" alt="plot of chunk unnamed-chunk-13" /></p>
<hr />
<p>Systematic departures from linearity can show up in the residual plots</p>
</div>
<div id="model-ii-regression---a-different-kind-of-residual" class="section level1">
<h1>Model II Regression - A different kind of residual</h1>
<p>RMA regression assumes there is error in both X and Y</p>
<p>Minimizes an orthogonal residual sum of squares (along the major axis)</p>
<p>Show on white board</p>
</div>
<div id="doing-model-ii-regression-in-r" class="section level1">
<h1>Doing Model II Regression in R</h1>
<p>incremental:false</p>
<p>Model II (aka RMA) regression requires a separate package <code>lmodel2</code></p>
<p>The function is different, but the call is still simple.</p>
<pre class="r"><code>library(lmodel2)
lmodel2(response~predictor)</code></pre>
</div>
<div id="regression-in-context" class="section level1">
<h1>Regression In Context</h1>
<p>type:section</p>
<p>Regression is just one (the simplest) of many types of linear models that can describe linear relationships.</p>
<p>The bigger group comprises the</p>
<div id="general-linear-model" class="section level2">
<h2>General Linear Model</h2>
</div>
</div>
<div id="general-linear-model-1" class="section level1">
<h1>General Linear Model</h1>
<p>Family of statistical models of the form:</p>
<p><span class="math">\[Y_i = \beta_0 + \beta_1X_i + \beta_2X_i +\ ... +\ \beta_nX_i  + \epsilon_i\]</span></p>
<p>where:</p>
<ul>
<li><span class="math">\(\beta_0\)</span> is the y-intercept (value of y where x= 0)</li>
<li><span class="math">\(\beta_1X_i\)</span> is the slope value for the 1st x variable</li>
<li><span class="math">\(\epsilon_i\)</span> is the error term</li>
</ul>
</div>
<div id="general-linear-model-2" class="section level1">
<h1>General Linear Model</h1>
<p>Includes many common statistical methods:</p>
<ul>
<li>Regression</li>
<li>Multiple Regression</li>
<li>ANOVA</li>
<li>ANCOVA</li>
</ul>
<p><strong>All of these use the same function in R, called <code>lm()</code></strong></p>
</div>
<div id="choosing-a-general-linear-model" class="section level1">
<h1>Choosing a General Linear Model</h1>
<p>First decide which are explanatory and response variables.</p>
<ul>
<li>All explanatory variables continuous
<ul>
<li><strong><em>Regression</em></strong></li>
</ul></li>
<li>All explanatory variables categorical
<ul>
<li><strong><em>ANOVA (Analysis of Variance)</em></strong></li>
</ul></li>
<li>Explanatory variables both continuous and categorical
<ul>
<li><strong><em>ANCOVA (Analysis of Covariance)</em></strong></li>
</ul></li>
</ul>
<p>￼</p>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>

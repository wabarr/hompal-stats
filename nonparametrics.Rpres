Non Parametric Stats
========================================================
incremental:true

## when your data don't meet the assumptions

```{r echo=FALSE}
qqnorm(rnorm(1000)^3, main="(ab)normal Q-Q plot")
```

WARNING - LOW POWER ZONE
=================
type: alert

If your data meet (or approximate) assumptions of parametrics, they are generally more powerful

Monte-Carlo techniques are also often more powerful than non-parametrics

However, non-parametrics simpler to use than MC

Rank-Order Statistics
============

Non-parametrics are known as rank-order tests, because they work by ranking observations and analyzing these ranks, rather than the data themselves.

To use non-parametrics with continuous values, you have to discard a lot of information.

We will talk about non-parametrics in relation to their parametric equivalents. 

Non-Parametric Regression
===========

Non-parametric regression techniques exist but are not commonly used.

There are, however, several non-parametric correlation techniques that are widely used.

Spearman's Rho
===========


X and Y values are ranked separately, and the Pearson's product-moment coefficient ($r$) is computed on these ranks

```{r}
x <- c(0.9, 6.8, 3.2  , 2.4, 1.2)
y <- c(0.1, 4.5, 5.4, 1.5, 1.9)
rank_x <- order(x)
rank_y <- order(y)
rank_x
rank_y

```

Spearman's Rho
============

```{r echo=FALSE}
par(mfrow=c(2,1))
plot(x, y, pch=16, main="Raw Data", cex.lab=1.5, cex=2, cex.main=2)
plot(rank_x,rank_y, pch=16, main="Ranked Data", cex.lab=1.5, cex=2, cex.main=2)
```

***

```{r}
cor(x, y, method = "pearson")
cor(x, y, method = "spearman")
cor(rank_x, rank_y, method = "pearson")
```

Kendall's Tau
=============

Alternative to Spearman....

*  Rank observations
*  Examine each pair of observations, determine whether they match or not
*  Compute $\tau$

$$\tau = \frac{(number\ of\ matched\ pairs) - (number\ of \ non\ matched\ pairs)}{\frac{1}{2}n(n-1)}$$

Note: the denominator is the total number of pairwise comparisons.

Kendall's Tau
===============
incremental: false

```{r echo=FALSE}
x <- rnorm(10, mean=10); 
y <- x * 0.7 + rnorm(10, sd=0.2) 
ggplot2::qplot(x, y, size=I(4)) + ggplot2::theme_bw(30)
```

***
```{r}
cor(x, y, method="kendall")
```

Non-Parametric t-test
===========

Mann-Whitney U, also known as the Wilcoxon Rank-Sum

*  rank observations, ignoring group
*  sum the ranks belonging to each group
*  calculate the test statistic

$$U = R - \frac{n(n+1)}{2}$$

*  $R$is the summed ranks, and $n$ is the group sample size
*  do this for both groups, and take the smallest as the test statistic
*  compare to known distribution under null hypothesis

Mann-Whitney U / Wilcoxon Rank-Sum
====================

```{r}
x <- rnorm(10, mean=5)
y <- rnorm(10, mean=7)
wilcox.test(x,y)
```

Non-Parametric ANOVA
===========

Kruskal-Wallis

*  Rank all observations
*  Calculate the average rank within each group
*  Compare the average rank within group the the overall average of ranks, using a weighted sum-of-squares technique
*  Compare p value of test statistic using chi-square approximation

Kruskal-Wallis
================
incremental: false

```{r}
var <- c(rnorm(20, mean=6), rnorm(20, mean=5)) 
group <- factor(c(rep("A", 20), rep("B", 20)))
ggplot2::qplot(y=var, x=group, geom="boxplot", fill=group) + 
  ggplot2::theme_bw(30)
```

Kruskal-Wallis
===============
```{r}
kruskal.test(var~group)
```

Goodness of Fit Test
============
incremental:false

Kolmogorov-Smirnov Test

Non-parametric test to determine whether two distributions differ

Based on theoretical vs empirical CDF
========
incremental:false

```{r echo=FALSE}
x <- rnorm(1000)
y <- x * 0.3 + rnorm(1000, sd=0.1)
resids <- resid(lm(y^2~x))
plot(ecdf(resids), main="Empirical CDF")

#trying to figure out how to get it on the same plot
#xrange <- seq(from = min(resids), to=max(resids), length.out = 100)
#densities <- pnorm(q = xrange , sd = sd(resids))
#points(x=xrange, y=densities)
```

*** 

```{r echo=FALSE}
plot(ecdf(rnorm(1000, sd=sd(resids))), main="Theoretical EDF")
```

KS-Test
=========
The single largest deviation of the empirical from the theoretical is the KS statistic. This is used to compute a p-value.

Can be used for any distribution, not just the normal distribution.

KS-Test in R
=========
```{r}
ks.test(rnorm(100), "punif")

```

KS-Test in R
=========
```{r}
ks.test(rnorm(100)^2, "pnorm")
```


Two Sample KS-Test in R
=========


```{r}
ks.test(runif(100), rnorm(100))
```

